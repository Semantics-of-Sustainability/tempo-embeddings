{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from importlib import reload\n",
    "\n",
    "reload(logging)\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s %(levelname)s:%(message)s\",\n",
    "    level=logging.INFO,\n",
    "    datefmt=\"%I:%M:%S\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install \"vegafusion[embed]>=1.5.0\"\n",
    "reload(logging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import altair as alt\n",
    "from ipywidgets import widgets  # type: ignore\n",
    "\n",
    "from tempo_embeddings.text.corpus import Corpus\n",
    "\n",
    "alt.data_transformers.enable(\"vegafusion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "from tempo_embeddings.embeddings.model import SentenceTransformerModelWrapper\n",
    "from tempo_embeddings.embeddings.weaviate_database import WeaviateDatabaseManager\n",
    "from tempo_embeddings.settings import DEFAULT_LANGUAGE_MODEL\n",
    "\n",
    "db = WeaviateDatabaseManager(\n",
    "    client=weaviate.connect_to_local(host=\"145.38.192.173\", port=8087),\n",
    "    model=SentenceTransformerModelWrapper.from_pretrained(DEFAULT_LANGUAGE_MODEL),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose from the available Collections in the Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_colls = list(db.get_available_collections())\n",
    "collection_selector = widgets.SelectMultiple(\n",
    "    options=existing_colls,\n",
    "    value=[\"ANP\", \"StatenGeneraal\"],\n",
    "    description=\"Choose a Collection:\",\n",
    "    disabled=False,\n",
    "    style={\"description_width\": \"initial\"},\n",
    ")\n",
    "\n",
    "print(\"\\nCollection Sizes\")\n",
    "print(\"----------------\")\n",
    "max_len = max(len(collection) for collection in existing_colls) + 1\n",
    "for collection in existing_colls:\n",
    "    print(f\"{collection:{max_len}}\\t{db.get_collection_count(collection)}\")\n",
    "\n",
    "collection_selector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Sub-Corpus\n",
    "\n",
    "To make the processing and visualization easier, we will create a new `Corpus` comprising only a subet of the original Collection. This corpus will contain only the records of interest. This is done by querying the database with keyword and metadata constraints. In this example we allow to look for:\n",
    "\n",
    "- **Filter Terms:** retrieve only passages that contain exactly the given keywords.\n",
    "- **Year Range:** retrieve only the records which are inside the provided years\n",
    "- **Neighbors:** This indicates how much to *expand* the search into more datapoints. The idea is to retrieve the *top_k* neighbors of the initially retrieved passages. Ideally this will give related passages that did not mention any of the keywords explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "widget_year_range = widgets.IntRangeSlider(\n",
    "    description=\"Year Range: \",\n",
    "    min=1800,\n",
    "    max=2020,\n",
    "    step=1,\n",
    "    value=(1950, 2000),\n",
    "    style={\"description_width\": \"initial\"},\n",
    "    layout=widgets.Layout(width=\"400px\"),\n",
    ")\n",
    "widget_terms = widgets.Text(\n",
    "    description=\"Filter Terms (comma separated)\",\n",
    "    value=\"duurzaam\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    "    layout=widgets.Layout(width=\"600px\"),\n",
    ")\n",
    "widget_neighbors = widgets.IntSlider(\n",
    "    description=\"Expand Neighborhood Size: \",\n",
    "    min=0,\n",
    "    max=10,\n",
    "    value=5,\n",
    "    style={\"description_width\": \"initial\"},\n",
    "    layout=widgets.Layout(width=\"400px\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the Widgets to choose the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(widget_terms)\n",
    "display(widget_year_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute the Search\n",
    "\n",
    "No need to move the code manually here. All parameters are grabbed from the widget values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# Unpack values form Widget\n",
    "year_from, year_to = widget_year_range.value\n",
    "FILTER_TERMS = [s.strip() for s in widget_terms.value.split(\",\")]\n",
    "# Execute Database Query\n",
    "where_range = {\"year_from\": year_from, \"year_to\": year_to}\n",
    "print(f\"Searching terms {FILTER_TERMS} between year {year_from} and {year_to}\")\n",
    "corpus = sum(\n",
    "    (\n",
    "        db.get_corpus(\n",
    "            collection,\n",
    "            filter_words=FILTER_TERMS,\n",
    "            where_obj=where_range,\n",
    "            include_embeddings=True,\n",
    "            limit=10000,\n",
    "        )\n",
    "        for collection in collection_selector.value\n",
    "    ),\n",
    "    start=Corpus([]),\n",
    ")\n",
    "print(f\"Found {len(corpus)} items that match!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display the Datapoints\n",
    "\n",
    "Here we only display what we got (After using UMAP to compress). The \"cluster\" colors are assigned based on the Year Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.compress_embeddings()\n",
    "print(corpus.embeddings.shape)\n",
    "print(corpus.embeddings_2d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "corpus_df = corpus.to_dataframe()\n",
    "corpus_df[\"year\"] = corpus_df[\"year\"].astype(int)\n",
    "corpus_df[\"decade\"] = (corpus_df[\"year\"] // 10) * 10\n",
    "corpus_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decades = corpus_df[\"decade\"].dropna().unique()\n",
    "decades.sort()\n",
    "colorSelector = alt.selection_point(\n",
    "    name=\"Select\", fields=[\"decade\"], value=decades[0], bind=\"legend\"\n",
    ")\n",
    "\n",
    "alt.Chart(corpus_df).mark_circle().add_params(colorSelector).encode(\n",
    "    x=\"x\",\n",
    "    y=\"y\",\n",
    "    tooltip=[\"text\", \"date\", \"provenance\"],\n",
    "    color={\n",
    "        \"field\": \"decade\",\n",
    "        \"scale\": {\"scheme\": \"category20b\"},\n",
    "        \"legend\": alt.Legend(\n",
    "            title=\"Year\", labelLimit=0, columns=1, labelFontSize=16, titleFontSize=18\n",
    "        ),\n",
    "    },\n",
    "    opacity=alt.condition(colorSelector, alt.value(0.95), alt.value(0.01)),\n",
    ").properties(width=1200, height=800).interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster the Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CASE 1: Clustering Independently Per DECADE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "    <style>\n",
       "        .bk-notebook-logo {\n",
       "            display: block;\n",
       "            width: 20px;\n",
       "            height: 20px;\n",
       "            background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAABx0RVh0U29mdHdhcmUAQWRvYmUgRmlyZXdvcmtzIENTNui8sowAAAOkSURBVDiNjZRtaJVlGMd/1/08zzln5zjP1LWcU9N0NkN8m2CYjpgQYQXqSs0I84OLIC0hkEKoPtiH3gmKoiJDU7QpLgoLjLIQCpEsNJ1vqUOdO7ppbuec5+V+rj4ctwzd8IIbbi6u+8f1539dt3A78eXC7QizUF7gyV1fD1Yqg4JWz84yffhm0qkFqBogB9rM8tZdtwVsPUhWhGcFJngGeWrPzHm5oaMmkfEg1usvLFyc8jLRqDOMru7AyC8saQr7GG7f5fvDeH7Ej8CM66nIF+8yngt6HWaKh7k49Soy9nXurCi1o3qUbS3zWfrYeQDTB/Qj6kX6Ybhw4B+bOYoLKCC9H3Nu/leUTZ1JdRWkkn2ldcCamzrcf47KKXdAJllSlxAOkRgyHsGC/zRday5Qld9DyoM4/q/rUoy/CXh3jzOu3bHUVZeU+DEn8FInkPBFlu3+nW3Nw0mk6vCDiWg8CeJaxEwuHS3+z5RgY+YBR6V1Z1nxSOfoaPa4LASWxxdNp+VWTk7+4vzaou8v8PN+xo+KY2xsw6une2frhw05CTYOmQvsEhjhWjn0bmXPjpE1+kplmmkP3suftwTubK9Vq22qKmrBhpY4jvd5afdRA3wGjFAgcnTK2s4hY0/GPNIb0nErGMCRxWOOX64Z8RAC4oCXdklmEvcL8o0BfkNK4lUg9HTl+oPlQxdNo3Mg4Nv175e/1LDGzZen30MEjRUtmXSfiTVu1kK8W4txyV6BMKlbgk3lMwYCiusNy9fVfvvwMxv8Ynl6vxoByANLTWplvuj/nF9m2+PDtt1eiHPBr1oIfhCChQMBw6Aw0UulqTKZdfVvfG7VcfIqLG9bcldL/+pdWTLxLUy8Qq38heUIjh4XlzZxzQm19lLFlr8vdQ97rjZVOLf8nclzckbcD4wxXMidpX30sFd37Fv/GtwwhzhxGVAprjbg0gCAEeIgwCZyTV2Z1REEW8O4py0wsjeloKoMr6iCY6dP92H6Vw/oTyICIthibxjm/DfN9lVz8IqtqKYLUXfoKVMVQVVJOElGjrnnUt9T9wbgp8AyYKaGlqingHZU/uG2NTZSVqwHQTWkx9hxjkpWDaCg6Ckj5qebgBVbT3V3NNXMSiWSDdGV3hrtzla7J+duwPOToIg42ChPQOQjspnSlp1V+Gjdged7+8UN5CRAV7a5EdFNwCjEaBR27b3W890TE7g24NAP/mMDXRWrGoFPQI9ls/MWO2dWFAar/xcOIImbbpA3zgAAAABJRU5ErkJggg==);\n",
       "        }\n",
       "    </style>\n",
       "    <div>\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-notebook-logo\"></a>\n",
       "        <span id=\"a33186dc-f3f1-4280-9a6a-e534c7360906\">Loading BokehJS ...</span>\n",
       "    </div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\nconst JS_MIME_TYPE = 'application/javascript';\n  const HTML_MIME_TYPE = 'text/html';\n  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n  const CLASS_NAME = 'output_bokeh rendered_html';\n\n  /**\n   * Render data to the DOM node\n   */\n  function render(props, node) {\n    const script = document.createElement(\"script\");\n    node.appendChild(script);\n  }\n\n  /**\n   * Handle when an output is cleared or removed\n   */\n  function handleClearOutput(event, handle) {\n    function drop(id) {\n      const view = Bokeh.index.get_by_id(id)\n      if (view != null) {\n        view.model.document.clear()\n        Bokeh.index.delete(view)\n      }\n    }\n\n    const cell = handle.cell;\n\n    const id = cell.output_area._bokeh_element_id;\n    const server_id = cell.output_area._bokeh_server_id;\n\n    // Clean up Bokeh references\n    if (id != null) {\n      drop(id)\n    }\n\n    if (server_id !== undefined) {\n      // Clean up Bokeh references\n      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n      cell.notebook.kernel.execute(cmd_clean, {\n        iopub: {\n          output: function(msg) {\n            const id = msg.content.text.trim()\n            drop(id)\n          }\n        }\n      });\n      // Destroy server and session\n      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n      cell.notebook.kernel.execute(cmd_destroy);\n    }\n  }\n\n  /**\n   * Handle when a new output is added\n   */\n  function handleAddOutput(event, handle) {\n    const output_area = handle.output_area;\n    const output = handle.output;\n\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n      return\n    }\n\n    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n      // store reference to embed id on output_area\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n    }\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n      const bk_div = document.createElement(\"div\");\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n      const script_attrs = bk_div.children[0].attributes;\n      for (let i = 0; i < script_attrs.length; i++) {\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n      }\n      // store reference to server id on output_area\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n    }\n  }\n\n  function register_renderer(events, OutputArea) {\n\n    function append_mime(data, metadata, element) {\n      // create a DOM node to render to\n      const toinsert = this.create_output_subarea(\n        metadata,\n        CLASS_NAME,\n        EXEC_MIME_TYPE\n      );\n      this.keyboard_manager.register_events(toinsert);\n      // Render to node\n      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n      render(props, toinsert[toinsert.length - 1]);\n      element.append(toinsert);\n      return toinsert\n    }\n\n    /* Handle when an output is cleared or removed */\n    events.on('clear_output.CodeCell', handleClearOutput);\n    events.on('delete.Cell', handleClearOutput);\n\n    /* Handle when a new output is added */\n    events.on('output_added.OutputArea', handleAddOutput);\n\n    /**\n     * Register the mime type and append_mime function with output_area\n     */\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n      /* Is output safe? */\n      safe: true,\n      /* Index of renderer in `output_area.display_order` */\n      index: 0\n    });\n  }\n\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\n  if (root.Jupyter !== undefined) {\n    const events = require('base/js/events');\n    const OutputArea = require('notebook/js/outputarea').OutputArea;\n\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  }\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    const el = document.getElementById(\"a33186dc-f3f1-4280-9a6a-e534c7360906\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.3.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.3.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.3.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.3.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.3.3.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n          for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\nif (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"a33186dc-f3f1-4280-9a6a-e534c7360906\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));",
      "application/vnd.bokehjs_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/jose/Repos/tempo-embeddings/.venv/lib/python3.9/site-packages/tempo_embeddings/data/stopwords-filter-nl.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 138\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclusters\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    137\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclusters\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 138\u001b[0m stopwords \u001b[38;5;241m=\u001b[39m \u001b[43mget_stopwords\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m TIME_BUCKETS \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;241m1950\u001b[39m, \u001b[38;5;241m1959\u001b[39m), (\u001b[38;5;241m1960\u001b[39m, \u001b[38;5;241m1969\u001b[39m), (\u001b[38;5;241m1970\u001b[39m, \u001b[38;5;241m1979\u001b[39m), (\u001b[38;5;241m1980\u001b[39m, \u001b[38;5;241m1989\u001b[39m), (\u001b[38;5;241m1990\u001b[39m, \u001b[38;5;241m1999\u001b[39m)]\n\u001b[1;32m    142\u001b[0m all_clusters_dict \u001b[38;5;241m=\u001b[39m {}\n",
      "Cell \u001b[0;32mIn[22], line 18\u001b[0m, in \u001b[0;36mget_stopwords\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_stopwords\u001b[39m():\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# stopwords_file = Path(\"../../tempo_embeddings/data/stopwords-filter-nl.txt\")\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     stopwords_file \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATA_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/stopwords-filter-nl.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstopwords_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabsolute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     19\u001b[0m         stopwords \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(f\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39msplitlines())\n\u001b[1;32m     20\u001b[0m     stopwords\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m     21\u001b[0m         {\n\u001b[1;32m     22\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwij\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m         }\n\u001b[1;32m     33\u001b[0m     )\n",
      "File \u001b[0;32m~/Repos/tempo-embeddings/.venv/lib/python3.9/site-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/jose/Repos/tempo-embeddings/.venv/lib/python3.9/site-packages/tempo_embeddings/data/stopwords-filter-nl.txt'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.plotting import show\n",
    "\n",
    "# from tempo_embeddings.settings import DATA_DIR\n",
    "from tempo_embeddings.text.passage import Highlighting, Passage\n",
    "from tempo_embeddings.visualization.bokeh import BokehInteractiveVisualizer\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "MIN_CLUSTER_SIZE = 50\n",
    "\n",
    "\n",
    "def get_stopwords():\n",
    "    stopwords_file = Path(\"../../tempo_embeddings/data/stopwords-filter-nl.txt\")\n",
    "    # stopwords_file = Path(f\"{DATA_DIR}/stopwords-filter-nl.txt\")\n",
    "    with open(stopwords_file.absolute(), \"rt\") as f:\n",
    "        stopwords = set(f.read().splitlines())\n",
    "    stopwords.update(\n",
    "        {\n",
    "            \"wij\",\n",
    "            \"we\",\n",
    "            \"moeten\",\n",
    "            \"heer\",\n",
    "            \"mevrouw\",\n",
    "            \"minister\",\n",
    "            \"voorzitter\",\n",
    "            \"gaat\",\n",
    "            \"wel\",\n",
    "            \"den\",\n",
    "        }\n",
    "    )\n",
    "    return stopwords\n",
    "\n",
    "\n",
    "def get_passages_from_df(df, skip_columns=[]):\n",
    "    passages = []\n",
    "    for row in df.to_dict(\"records\"):\n",
    "        meta = {k: v for (k, v) in row.items() if k not in skip_columns}\n",
    "        passages.append(\n",
    "            Passage(row[\"text\"], metadata=meta, highlighting=Highlighting(1, 10))\n",
    "        )\n",
    "    return passages\n",
    "\n",
    "\n",
    "def cluster_selected_corpus(year_start, year_end, min_cluster_size):\n",
    "    corpus_df_filtered = corpus_df[\n",
    "        (corpus_df[\"year\"] >= year_start) & (corpus_df[\"year\"] <= year_end)\n",
    "    ]\n",
    "    corpus_to_cluster = Corpus(\n",
    "        get_passages_from_df(corpus_df_filtered, skip_columns=[\"text\"])\n",
    "    )\n",
    "    corpus_to_cluster.embeddings = list(\n",
    "        zip(corpus_df_filtered[\"x\"], corpus_df_filtered[\"y\"])\n",
    "    )\n",
    "    print(\n",
    "        f\"Clustering only {len(corpus_to_cluster)} datapoints between {year_start} and {year_end}...\"\n",
    "    )\n",
    "    clusters = corpus_to_cluster.cluster(\n",
    "        min_cluster_size=min_cluster_size, cluster_selection_epsilon=0.1\n",
    "    )\n",
    "    print(\n",
    "        f\"Found {len(clusters)} clusters in the corpus. (min cluster size is {min_cluster_size})\"\n",
    "    )\n",
    "    if len(clusters) > 100:\n",
    "        raise ValueError(\n",
    "            \"Seems like you have too many clusters! Try with a bigger value for min_cluster_size to avoid memory issues\"\n",
    "        )\n",
    "    [\n",
    "        cl.set_topic_label(exclude_words=frozenset(stopwords | set(FILTER_TERMS)), n=5)\n",
    "        for cl in clusters\n",
    "    ]\n",
    "    # Return Clusters Plot\n",
    "    return clusters\n",
    "\n",
    "\n",
    "def bokeh_plot_clustered_corpus(clusters):\n",
    "    meta_fields = corpus.metadata_fields()\n",
    "    meta_fields = [\"year\", \"date\", \"issue\", \"provenance\"]\n",
    "\n",
    "    visualizer = BokehInteractiveVisualizer(\n",
    "        *clusters, metadata_fields=meta_fields, width=1000, height=500\n",
    "    )\n",
    "\n",
    "    os.environ[\"BOKEH_ALLOW_WS_ORIGIN\"] = \"*\"\n",
    "\n",
    "    show(visualizer.create_document)\n",
    "\n",
    "\n",
    "def altair_plot_clustered_corpus(clusters):\n",
    "    # Prepare Data to Plot\n",
    "    plot_data = clusters[0].to_dataframe()\n",
    "    plot_data[\"cluster_label\"] = clusters[0].label\n",
    "    cl_labels = [clusters[0].label]\n",
    "    if len(clusters) > 1:\n",
    "        for cl in clusters[1:]:\n",
    "            cl_labels.append(cl.label)\n",
    "            df = cl.to_dataframe()\n",
    "            df[\"cluster_label\"] = cl.label\n",
    "            plot_data = pd.concat([plot_data, df])\n",
    "        plot_data[[\"x\", \"y\"]] = pd.DataFrame(\n",
    "            plot_data[\"datapoint\"].tolist(), index=plot_data.index\n",
    "        )\n",
    "    plot_data = plot_data.drop(columns=[\"datapoint\"])\n",
    "    # Altair Chart\n",
    "    colorSelector = alt.selection_point(\n",
    "        name=\"Select\", fields=[\"cluster_label\"], value=cl_labels[0], bind=\"legend\"\n",
    "    )\n",
    "    return (\n",
    "        alt.Chart(plot_data)\n",
    "        .mark_circle()\n",
    "        .add_params(colorSelector)\n",
    "        .encode(\n",
    "            x=\"x\",\n",
    "            y=\"y\",\n",
    "            tooltip=[\"text\", \"date\", \"provenance\"],\n",
    "            color={\n",
    "                \"field\": \"cluster_label\",\n",
    "                # \"scale\": {\"scheme\": \"category20b\"},\n",
    "                \"legend\": alt.Legend(\n",
    "                    title=\"Cluster Label\",\n",
    "                    labelLimit=0,\n",
    "                    columns=1,\n",
    "                    labelFontSize=16,\n",
    "                    titleFontSize=18,\n",
    "                ),\n",
    "            },\n",
    "            opacity=alt.condition(colorSelector, alt.value(0.95), alt.value(0.01)),\n",
    "        )\n",
    "        .properties(width=1200, height=800)\n",
    "        .interactive()\n",
    "    )\n",
    "\n",
    "\n",
    "if not os.path.exists(\"clusters\"):\n",
    "    os.makedirs(\"clusters\")\n",
    "stopwords = get_stopwords()\n",
    "\n",
    "TIME_BUCKETS = [(1950, 1959), (1960, 1969), (1970, 1979), (1980, 1989), (1990, 1999)]\n",
    "\n",
    "all_clusters_dict = {}\n",
    "for decade in TIME_BUCKETS:\n",
    "    clusters = cluster_selected_corpus(*decade, MIN_CLUSTER_SIZE)\n",
    "    bokeh_plot_clustered_corpus(clusters)\n",
    "    # display(altair_plot_clustered_corpus(clusters))\n",
    "    all_clusters_dict[decade] = clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_count(corpus):\n",
    "    word_count = Counter()\n",
    "    [\n",
    "        word_count.update([w for w in p.words() if w not in stopwords])\n",
    "        for p in corpus.passages\n",
    "    ]\n",
    "    return word_count\n",
    "\n",
    "\n",
    "def analyze_clusters(clusters):\n",
    "    # selected_metadata = [\"year\"] #  \"top_words\"\n",
    "    # df_cluster_meta = []\n",
    "    global_top_words = Counter()\n",
    "    for cluster in sorted(clusters, key=lambda c: len(c.passages), reverse=True):\n",
    "        cluster.set_topic_label(\n",
    "            exclude_words=frozenset(stopwords | set(FILTER_TERMS)), n=5\n",
    "        )\n",
    "        top_words = cluster.top_words(\n",
    "            exclude_words=frozenset(stopwords | set(FILTER_TERMS)), n=20\n",
    "        )\n",
    "        print(\n",
    "            f\"\\n----- Cluster {cluster.label} || Size = {len(cluster.passages)} -----\\n\\tTop Words: {', '.join(sorted(top_words))}\"\n",
    "        )\n",
    "        word_counts = get_word_count(cluster)\n",
    "        print(word_counts.most_common(10))\n",
    "        global_top_words += word_counts\n",
    "    return global_top_words\n",
    "\n",
    "\n",
    "for decade, clusters in all_clusters_dict.items():\n",
    "    print(f\"\\n\\n\\n################### DECADE {decade} ###################\\n\")\n",
    "    print(f\"Total Clusters = {len(clusters)}\")\n",
    "    global_top_words = analyze_clusters(clusters)\n",
    "    print(f\"Decade Most Common Words: {global_top_words.most_common(20)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CASE 2: Cluster Everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = cluster_selected_corpus(*widget_year_range.value, min_cluster_size=100)\n",
    "# altair_plot_clustered_corpus(clusters)\n",
    "bokeh_plot_clustered_corpus(clusters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
