{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dutch Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install tempo-embeddings from GitHub\n",
    "# This can also refer to a specific version or branch\n",
    "\n",
    "#%pip install --upgrade pip  # Required for properly resolving dependencies\n",
    "#%pip uninstall -y tempo_embeddings  # Remove existing installation\n",
    "#%pip install --upgrade git+https://github.com/Semantics-of-Sustainability/tempo-embeddings.git\n",
    "%pip install -e ../ # to pull the \"local\" tempo embeddings and automatically register code changes when debugging\n",
    "%pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure installation has succeeded\n",
    "import tempo_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "\n",
    "    IN_COLAB = True\n",
    "except ModuleNotFoundError:\n",
    "    IN_COLAB = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "The data needs to be downloaded and provided in the path configured in the next cell.\n",
    "\n",
    "NOTE: You have to manually adapt the `DATA_DIR` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload now\n",
    "\n",
    "import operator\n",
    "from functools import reduce\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from tempo_embeddings.text.corpus import Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 200\n",
    "\n",
    "RANDOM_SAMPLE_ANP = 200\n",
    "RANDOM_SAMPLE_STATEN_GENERAAL = 200\n",
    "\n",
    "STATEN_GENERAAL_BLACKLIST = [\"1987\"]\n",
    "\n",
    "FILTER_TERMS = [\"duurzaam\"]  # Search term(s) for filtering the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: Adapt the `DATA_DIR` below manually!\n",
    "## For a shared Google Drive, create a shortcut into your own Google Drive\n",
    "## See https://stackoverflow.com/questions/54351852/accessing-shared-with-me-with-colab\n",
    "\n",
    "LOCAL_PATHS: list[Path] = [\n",
    "    Path.home() / \"Documents\" / \"SemanticsOfSustainability\" / \"data\" / \"Joris\",\n",
    "    Path.home() / \"SEED_DATA\" / \"SemanticsSustainability\", # local angel\n",
    "    Path(\"/data/volume_2/data\"),  # Research Cloud\n",
    "    Path(\"/home/cschnober/data/\"),  # Snellius\n",
    "]\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount(\"/content/drive\")\n",
    "\n",
    "    DATA_DIR = Path(\"/content/drive/MyDrive/Data/\")\n",
    "else:\n",
    "    try:\n",
    "        DATA_DIR = next(path for path in LOCAL_PATHS if path.is_dir())\n",
    "    except StopIteration as e:\n",
    "        raise DirectoryNotFoundError(f\"Data directory not found.\") from e\n",
    "\n",
    "assert DATA_DIR.is_dir(), f\"Data dir '{DATA_DIR}' not found.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANP_DIR = DATA_DIR / \"ANP\"\n",
    "assert RANDOM_SAMPLE_ANP == 0 or ANP_DIR.is_dir(), f\"{ANP_DIR} not found.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "anp_files = list(ANP_DIR.glob(\"ANP_????.csv.gz\"))\n",
    "\n",
    "if RANDOM_SAMPLE_ANP and len(anp_files) > RANDOM_SAMPLE_ANP:\n",
    "    anp_files = random.sample(\n",
    "        list(ANP_DIR.glob(\"ANP_????.csv.gz\")), k=RANDOM_SAMPLE_ANP\n",
    "    )\n",
    "\n",
    "print(f\"Found {len(anp_files)} ANP Files\")\n",
    "anp_files[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anp_corpus = (\n",
    "    reduce(\n",
    "        operator.add,\n",
    "        (\n",
    "            Corpus.from_csv_file(\n",
    "                path,\n",
    "                filter_terms=FILTER_TERMS,\n",
    "                text_columns=[\"content\"],\n",
    "                encoding=\"iso8859_15\",\n",
    "                compression=\"gzip\",\n",
    "                delimiter=\";\",\n",
    "                window_size=WINDOW_SIZE,\n",
    "            )\n",
    "            for path in tqdm(anp_files, unit=\"file\")\n",
    "        ),\n",
    "    )\n",
    "    if anp_files\n",
    "    else Corpus()\n",
    ")\n",
    "\n",
    "len(anp_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Staten Generaal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATEN_GENERAAL_DIR = DATA_DIR / \"StatenGeneraal\"\n",
    "\n",
    "assert RANDOM_SAMPLE_STATEN_GENERAAL == 0 or STATEN_GENERAAL_DIR.is_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glob195x = \"StatenGeneraal_19[0-9]?.csv.gz\"  # Pattern for files from 1950-1999\n",
    "glob20xx = \"StatenGeneraal_2???.csv.gz\"  # Pattern for files from 2000\n",
    "\n",
    "files_195x = list(STATEN_GENERAAL_DIR.glob(glob195x))\n",
    "files_20xx = list(STATEN_GENERAAL_DIR.glob(glob20xx))\n",
    "\n",
    "sg_files = [\n",
    "    file\n",
    "    # Merge files from patterns\n",
    "    for file in files_20xx + files_195x\n",
    "    # Remove blacklisted files:\n",
    "    for blacklisted in STATEN_GENERAAL_BLACKLIST\n",
    "    if blacklisted not in file.name\n",
    "]\n",
    "\n",
    "if RANDOM_SAMPLE_STATEN_GENERAAL and RANDOM_SAMPLE_STATEN_GENERAAL < len(sg_files):\n",
    "    sg_files = random.sample(sg_files, k=RANDOM_SAMPLE_STATEN_GENERAAL)\n",
    "\n",
    "print(f\"Found {len(sg_files)} STAATEN_G Files\")\n",
    "sorted(sg_files[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload now\n",
    "\n",
    "import csv\n",
    "\n",
    "csv.field_size_limit(100000000)\n",
    "\n",
    "sg_corpus = (\n",
    "    reduce(\n",
    "        operator.add,\n",
    "        (\n",
    "            Corpus.from_csv_file(\n",
    "                path,\n",
    "                filter_terms=FILTER_TERMS,\n",
    "                text_columns=[\"Content\"],\n",
    "                encoding=\"utf-8\",\n",
    "                compression=\"gzip\",\n",
    "                delimiter=\";\",\n",
    "                window_size=WINDOW_SIZE,\n",
    "            )\n",
    "            for path in tqdm(sg_files, unit=\"file\")\n",
    "        ),\n",
    "    )\n",
    "    if sg_files\n",
    "    else Corpus()\n",
    ")\n",
    "\n",
    "len(sg_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in sg_corpus.passages[:20]:\n",
    "    print(len(p), p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = anp_corpus + sg_corpus\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create or Open existing Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"NetherlandsForensicInstitute/robbert-2022-dutch-sentence-transformers\"\n",
    "EMBED_CONFIG = {\"type\":\"hf\", \"api_key\": \"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\"}\n",
    "\n",
    "from tempo_embeddings.embeddings.vector_database import ChromaDatabaseManager\n",
    "db = ChromaDatabaseManager(db_path=\"testing_db\", embedder_name=MODEL_NAME,embedder_config=EMBED_CONFIG, batch_size=10)\n",
    "db.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload now\n",
    "\n",
    "collection_name = \"anp_sg_corpus\"\n",
    "loaded_existing_collection = False\n",
    "\n",
    "anp_sg_collection = db.create_new_collection(collection_name, corpus.passages)\n",
    "\n",
    "if not anp_sg_collection:\n",
    "    anp_sg_collection = db.get_existing_collection(collection_name)\n",
    "    loaded_existing_collection = True\n",
    "\n",
    "# This hack should be improved. We need to re-think the whole tokenization process...\n",
    "if loaded_existing_collection:\n",
    "    for p in corpus.passages:\n",
    "        p.tokenization = db._tokenize(p.text)\n",
    "\n",
    "# Generate 2-Dim Embeddings that will be clustered later\n",
    "two_dim_embeddings = db.compress_embeddings(anp_sg_collection, persist_in_db=True)\n",
    "if two_dim_embeddings is not None:\n",
    "    corpus.embeddings = two_dim_embeddings\n",
    "    print(len(corpus.embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TESTS: Retrieve Records from Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = db.get_records(anp_sg_collection, filter_words=[\"toekomst\"], where_obj={'$and': [{'year': {'$eq': '1983'}}, {'month': {'$eq': '7'}}]})\n",
    "\n",
    "print(records[\"ids\"])\n",
    "\n",
    "for x in records[\"documents\"][:10]:\n",
    "    print(x)\n",
    "    print(db.is_in_collection(anp_sg_collection, x))\n",
    "    print(db.get_vector_from_db(anp_sg_collection, x))\n",
    "    print(db.embed_text_batch([x])[0])\n",
    "    print(\"-----\")\n",
    "\n",
    "print(len(records[\"ids\"]))\n",
    "print([len(x) for x in corpus.embeddings[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = db.query_text_neighbors(anp_sg_collection, \"Duurzaamheid en toekomst!\")\n",
    "\n",
    "print(records[\"ids\"])\n",
    "\n",
    "for i in range(len(records[\"ids\"])):\n",
    "    print(records[\"documents\"][i])\n",
    "    print(records[\"distances\"][i])\n",
    "    print(\"-----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = db.embed_text_batch([\"Duurzaamheid en toekomst!\"])[0]\n",
    "records = db.query_vector_neighbors(anp_sg_collection, vector)\n",
    "\n",
    "print(records[\"ids\"])\n",
    "\n",
    "for i in range(len(records[\"ids\"])):\n",
    "    print(records[\"documents\"][i])\n",
    "    print(records[\"distances\"][i])\n",
    "    print(\"-----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --continue https://raw.githubusercontent.com/Semantics-of-Sustainability/tempo-embeddings/main/tempo_embeddings/data/stopwords-filter-nl.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_file = Path(\"stopwords-filter-nl.txt\")\n",
    "\n",
    "with open(stopwords_file.absolute(), \"rt\") as f:\n",
    "    stopwords = set(f.read().splitlines())\n",
    "\n",
    "stopwords.update(\n",
    "    {\n",
    "        \"wij\",\n",
    "        \"we\",\n",
    "        \"moeten\",\n",
    "        \"heer\",\n",
    "        \"mevrouw\",\n",
    "        \"minister\",\n",
    "        \"voorzitter\",\n",
    "        \"gaat\",\n",
    "        \"wel\",\n",
    "        \"den\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload now\n",
    "\n",
    "# Arguments: min_cluster_size=10, cluster_selection_epsilon=0.1, ...\n",
    "# See https://scikit-learn.org/stable/modules/generated/sklearn.cluster.HDBSCAN.html for full list\n",
    "\n",
    "# e.g. min_samples=10, cluster_selection_epsilon=0.2, cluster_selection_method=\"leaf\"\n",
    "\n",
    "clusters = corpus.cluster(min_cluster_size=10, cluster_selection_epsilon=0.1)\n",
    "print(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster in clusters:\n",
    "    cluster.set_topic_label(exclude_words=frozenset(stopwords | set(FILTER_TERMS)), n=5)\n",
    "    print(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"clusters.txt\", \"wt\") as f:\n",
    "    for cluster in clusters:\n",
    "        print(\n",
    "            \", \".join(\n",
    "                cluster.top_words(\n",
    "                    exclude_words=frozenset(stopwords | set(FILTER_TERMS)), n=5\n",
    "                )\n",
    "            ),\n",
    "            file=f,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload now\n",
    "\n",
    "from tempo_embeddings.visualization.clusters import ClusterVisualizer\n",
    "\n",
    "visualizer = ClusterVisualizer(*clusters)\n",
    "visualizer.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.io import reset_output\n",
    "from bokeh.plotting import show\n",
    "from tempo_embeddings.visualization.bokeh import BokehInteractiveVisualizer\n",
    "\n",
    "\n",
    "output_notebook()\n",
    "# reset_output()\n",
    "\n",
    "visualizer = BokehInteractiveVisualizer(\n",
    "    *clusters, metadata_fields=corpus.metadata_fields(), width=2000, height=1000\n",
    ")\n",
    "\n",
    "os.environ[\n",
    "    \"BOKEH_ALLOW_WS_ORIGIN\"\n",
    "] = \"*\"#\"0gvbv9d871k7g8j4h6mppna69o1qlh79dr9fepnuo1qr04mk1hbe\"\n",
    "\n",
    "# NOTE: Bookeh Runs in the 5006 PORT By default...\n",
    "\n",
    "show(visualizer.create_document)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
