{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dutch Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install tempo-embeddings from GitHub\n",
    "# This can also refer to a specific version or branch\n",
    "# %pip install ..\n",
    "\n",
    "%pip install --upgrade pip  # Required for properly resolving dependencies\n",
    "%pip uninstall -y tempo_embeddings  # Remove existing installation\n",
    "%pip install --upgrade git+https://github.com/Semantics-of-Sustainability/tempo-embeddings.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure installation has succeeded\n",
    "import tempo_embeddings\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import stanza\n",
    "# stanza.download('nl')\n",
    "# nlp_pipeline = stanza.Pipeline(\"nl\", processors='tokenize')\n",
    "nlp_pipeline = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "\n",
    "    IN_COLAB = True\n",
    "except ModuleNotFoundError:\n",
    "    IN_COLAB = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "The data needs to be downloaded and provided in the path configured in the next cell.\n",
    "\n",
    "NOTE: You have to manually adapt the `DATA_DIR` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload now\n",
    "\n",
    "import operator\n",
    "from functools import reduce\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from tempo_embeddings.text.corpus import Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 1000\n",
    "USE_FULL_SENTENCES = False # For now, this parameter overrides the window size\n",
    "\n",
    "RANDOM_SAMPLE_ANP = 200\n",
    "RANDOM_SAMPLE_STATEN_GENERAAL = 200\n",
    "\n",
    "STATEN_GENERAAL_BLACKLIST = [\"1987\"]\n",
    "\n",
    "FILTER_TERMS = [\"duurzaam\"]  # Search term(s) for filtering the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: Adapt the `DATA_DIR` below manually!\n",
    "## For a shared Google Drive, create a shortcut into your own Google Drive\n",
    "## See https://stackoverflow.com/questions/54351852/accessing-shared-with-me-with-colab\n",
    "\n",
    "LOCAL_PATHS: list[Path] = [\n",
    "    Path.home() / \"Documents\" / \"SemanticsOfSustainability\" / \"data\" / \"Joris\",\n",
    "    Path.home() / \"SEED_DATA\" / \"SemanticsSustainability\", # local angel\n",
    "    Path(\"/data/volume_2/data\"),  # Research Cloud\n",
    "    Path(\"/data/storage-semantics-of-sustainability/data/\"), # New Research Cloud\n",
    "    Path(\"/home/cschnober/data/\"),  # Snellius\n",
    "]\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount(\"/content/drive\")\n",
    "\n",
    "    DATA_DIR = Path(\"/content/drive/MyDrive/Data/\")\n",
    "else:\n",
    "    try:\n",
    "        DATA_DIR = next(path for path in LOCAL_PATHS if path.is_dir())\n",
    "    except StopIteration as e:\n",
    "        raise DirectoryNotFoundError(f\"Data directory not found.\") from e\n",
    "\n",
    "assert DATA_DIR.is_dir(), f\"Data dir '{DATA_DIR}' not found.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANP_DIR = DATA_DIR / \"ANP\"\n",
    "assert RANDOM_SAMPLE_ANP == 0 or ANP_DIR.is_dir(), f\"{ANP_DIR} not found.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "anp_files = list(ANP_DIR.glob(\"ANP_????.csv.gz\"))\n",
    "\n",
    "if RANDOM_SAMPLE_ANP and len(anp_files) > RANDOM_SAMPLE_ANP:\n",
    "    anp_files = random.sample(\n",
    "        list(ANP_DIR.glob(\"ANP_????.csv.gz\")), k=RANDOM_SAMPLE_ANP\n",
    "    )\n",
    "\n",
    "print(f\"Found {len(anp_files)} ANP Files\")\n",
    "anp_files[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anp_corpus = (\n",
    "    reduce(\n",
    "        operator.add,\n",
    "        (\n",
    "            Corpus.from_csv_file(\n",
    "                path,\n",
    "                filter_terms=FILTER_TERMS,\n",
    "                text_columns=[\"content\"],\n",
    "                encoding=\"iso8859_15\",\n",
    "                compression=\"gzip\",\n",
    "                delimiter=\";\",\n",
    "                window_size=WINDOW_SIZE,\n",
    "                nlp_pipeline=nlp_pipeline\n",
    "            )\n",
    "            for path in tqdm(anp_files, unit=\"file\")\n",
    "        ),\n",
    "    )\n",
    "    if anp_files\n",
    "    else Corpus()\n",
    ")\n",
    "\n",
    "len(anp_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Staten Generaal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATEN_GENERAAL_DIR = DATA_DIR / \"StatenGeneraal\"\n",
    "\n",
    "assert RANDOM_SAMPLE_STATEN_GENERAAL == 0 or STATEN_GENERAAL_DIR.is_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glob195x = \"StatenGeneraal_19[0-9]?.csv.gz\"  # Pattern for files from 1950-1999\n",
    "glob20xx = \"StatenGeneraal_2???.csv.gz\"  # Pattern for files from 2000\n",
    "\n",
    "files_195x = list(STATEN_GENERAAL_DIR.glob(glob195x))\n",
    "files_20xx = list(STATEN_GENERAAL_DIR.glob(glob20xx))\n",
    "\n",
    "sg_files = [\n",
    "    file\n",
    "    # Merge files from patterns\n",
    "    for file in files_20xx + files_195x\n",
    "    # Remove blacklisted files:\n",
    "    for blacklisted in STATEN_GENERAAL_BLACKLIST\n",
    "    if blacklisted not in file.name\n",
    "]\n",
    "\n",
    "if RANDOM_SAMPLE_STATEN_GENERAAL and RANDOM_SAMPLE_STATEN_GENERAAL < len(sg_files):\n",
    "    sg_files = random.sample(sg_files, k=RANDOM_SAMPLE_STATEN_GENERAAL)\n",
    "\n",
    "print(f\"Found {len(sg_files)} STAATEN_G Files\")\n",
    "sorted(sg_files[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload now\n",
    "\n",
    "import csv\n",
    "\n",
    "csv.field_size_limit(100000000)\n",
    "\n",
    "sg_corpus = (\n",
    "    reduce(\n",
    "        operator.add,\n",
    "        (\n",
    "            Corpus.from_csv_file(\n",
    "                path,\n",
    "                filter_terms=FILTER_TERMS,\n",
    "                text_columns=[\"Content\"],\n",
    "                encoding=\"utf-8\",\n",
    "                compression=\"gzip\",\n",
    "                delimiter=\";\",\n",
    "                window_size=WINDOW_SIZE,\n",
    "                nlp_pipeline=nlp_pipeline\n",
    "            )\n",
    "            for path in tqdm(sg_files, unit=\"file\")\n",
    "        ),\n",
    "    )\n",
    "    if sg_files\n",
    "    else Corpus()\n",
    ")\n",
    "\n",
    "len(sg_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in sg_corpus.passages[:20]:\n",
    "    print(len(p), p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = anp_corpus + sg_corpus\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload now\n",
    "\n",
    "LAYER = 9\n",
    "\n",
    "from tempo_embeddings.embeddings.model import EmbeddingsMethod\n",
    "\n",
    "from tempo_embeddings.embeddings.model import (\n",
    "    RobertaModelWrapper,\n",
    "    TransformerModelWrapper,\n",
    "    XModModelWrapper,\n",
    "    SentenceTransformerModelWrapper,\n",
    ")\n",
    "\n",
    "kwargs = {\"accelerate\": True}\n",
    "\n",
    "### RoBERTa Models\n",
    "# MODEL_NAME = \"DTAI-KULeuven/robbertje-1-gb-non-shuffled\"\n",
    "# model_class = RobertaModelWrapper\n",
    "\n",
    "### XMod Models\n",
    "# MODEL_NAME = \"facebook/xmod-base\"\n",
    "# kwargs[\"default_language\"] = \"nl_XX\"\n",
    "# model_class = XModModelWrapper\n",
    "\n",
    "### BERT Models\n",
    "# MODEL_NAME = \"GroNLP/bert-base-dutch-cased\"\n",
    "# MODEL_NAME = \"xlm-roberta-base\"\n",
    "# MODEL_NAME = \"xlm-mlm-100-1280\"\n",
    "# model_class = TransformerModelWrapper\n",
    "\n",
    "### Sentence Transformers\n",
    "MODEL_NAME = \"NetherlandsForensicInstitute/robbert-2022-dutch-sentence-transformers\"\n",
    "# MODEL_NAME = \"textgain/allnli-GroNLP-bert-base-dutch-cased\"\n",
    "model_class = SentenceTransformerModelWrapper\n",
    "\n",
    "model = model_class.from_pretrained(MODEL_NAME, layer=LAYER, **kwargs)\n",
    "model.embeddings_method = EmbeddingsMethod.MEAN\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create or Open existing Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tempo_embeddings.embeddings.vector_database import ChromaDatabaseManager\n",
    "\n",
    "EMBED_CONFIG = {\"type\":\"custom_model\", \"model\": model}\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "db = ChromaDatabaseManager(db_path=\"testing_db\", embedder_name=MODEL_NAME,embedder_config=EMBED_CONFIG, batch_size=BATCH_SIZE)\n",
    "db.connect()\n",
    "\n",
    "collection_name = \"anp_sg_corpus\"\n",
    "\n",
    "try:\n",
    "    anp_sg_collection = db.create_new_collection(collection_name)\n",
    "    created_new_collection = True\n",
    "except ValueError:\n",
    "    anp_sg_collection = db.get_existing_collection(collection_name)\n",
    "    created_new_collection = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute & Save Embeddings (If DB didn't existed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if created_new_collection:\n",
    "    # Insert the passages from the corpus in the new collection\n",
    "    db.insert_corpus(anp_sg_collection, corpus)\n",
    "else:\n",
    "    print(\"No new passages to insert\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST: Retrieve Records from Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload now\n",
    "\n",
    "# corpus = db.get_corpus(anp_sg_collection, filter_words=[\"toekomst\"], where_obj={'$and': [{'year': {'$eq': '1983'}}, {'month': {'$eq': '7'}}]})\n",
    "# corpus = db.get_corpus(anp_sg_collection, limit=100)\n",
    "corpus = db.get_corpus(anp_sg_collection, filter_words=[\"duurzaam\"])\n",
    "\n",
    "for p in corpus.passages:\n",
    "    print(p.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(corpus):\n",
    "    db.create_new_collection(\"anp_duurzam\", corpus)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
