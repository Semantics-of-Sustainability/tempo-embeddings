{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install tempo-embeddings from GitHub\n",
    "# # This can also refer to a specific version or branch\n",
    "\n",
    "# %pip install --upgrade pip  # Required for properly resolving dependencies\n",
    "# %pip uninstall -y tempo_embeddings  # Remove existing installation\n",
    "# %pip install --upgrade git+https://github.com/Semantics-of-Sustainability/tempo-embeddings.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure installation has succeeded\n",
    "import tempo_embeddings\n",
    "\n",
    "from importlib import reload\n",
    "import logging\n",
    "reload(logging)\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s:%(message)s', level=logging.INFO, datefmt='%I:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "\n",
    "    IN_COLAB = True\n",
    "except ModuleNotFoundError:\n",
    "    IN_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload now\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from tempo_embeddings.text.corpus import Corpus\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Database Manager\n",
    "\n",
    "The `db_path` parameter should point to the directory where the database is, so the original configuration and records are loaded. The database was created using the notebook `1_compute_embeddings_nl.ipynb`. If the given path does not exist, a new EMPTY database will be created there. \n",
    "\n",
    "A bigger `batch_size` could make the search faster but if it is too big you might run out of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jose/Repos/tempo-embeddings/.venv/lib/python3.9/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "11:20:15 INFO:Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "11:20:15 INFO:Path 'testing_db' already exists. Loading DB configuration:\n",
      "{'embedder_name': 'NetherlandsForensicInstitute/robbert-2022-dutch-sentence-transformers', 'embedder_type': 'custom_model', 'existing_collections': ['anp_sg_corpus', 'anp_duurzam']}\n"
     ]
    }
   ],
   "source": [
    "from tempo_embeddings.embeddings.vector_database import ChromaDatabaseManager\n",
    "\n",
    "# Here we load only the ANP collection because metadata field names diverge across datasets\n",
    "db = ChromaDatabaseManager(db_path=\"testing_db\", batch_size=8)\n",
    "db.connect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose from the available Collections in the Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84fbee092080413881797d74bdb5eba4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Choose a Collection:', options=('anp_sg_corpus', 'anp_duurzam'), style=DescriptionStyle(…"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "existing_colls = db.get_available_collections()\n",
    "collection_selector = widgets.Dropdown(\n",
    "    options=existing_colls,\n",
    "    description='Choose a Collection:',\n",
    "    disabled=False,\n",
    "    style={'description_width': 'initial'} \n",
    ")\n",
    "collection_selector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show number of records in the selected collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:22:09 INFO:Retrieved existing collection 'anp_duurzam'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collection 'anp_duurzam' has 1464 records\n"
     ]
    }
   ],
   "source": [
    "collection = db.get_existing_collection(collection_selector.value)\n",
    "print(f\"\\nCollection '{collection_selector.value}' has {collection.count()} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Sub-Corpus\n",
    "\n",
    "To make the processing and visualization easier, we will create a new `Corpus` comprising only a subet of the original Collection. This corpus will contain only the records of interest. This is done by querying the database with keyword and metadata constraints. In this example we allow to look for:\n",
    "\n",
    "- **Filter Terms:** retrieve only passages that contain exactly the given keywords.\n",
    "- **Year Range:** retrieve only the records which are inside the provided years\n",
    "- **Neighbors:** This indicates how much to *expand* the search into more datapoints. The idea is to retrieve the *top_k* neighbors of the initially retrieved passages. Ideally this will give related passages that did not mention any of the keywords explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "widget_year_range=widgets.IntRangeSlider(description='Year Range: ', min=1900, max=2020, step=1, value=(1980,1984), style={'description_width': 'initial'}, layout=widgets.Layout(width='400px') )\n",
    "widget_terms=widgets.Text(description='Filter Terms (comma separated)', value=\"duurzaam\", style={'description_width': 'initial'}, layout=widgets.Layout(width='600px') )\n",
    "widget_neighbors=widgets.IntSlider(description=\"Expand Neighborhood Size: \", min=0, max=10, value=5, style={'description_width': 'initial'}, layout=widgets.Layout(width='400px') )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the Widgets to choose the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12d230f26d904adcbede62c3cfb22985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='duurzaam', description='Filter Terms (comma separated)', layout=Layout(width='600px'), style=TextS…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "101a99c81ede4a59a370d114684f1554",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntRangeSlider(value=(1980, 1984), description='Year Range: ', layout=Layout(width='400px'), max=2020, min=190…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "055b5b8e9b9e48ac99e7b357498c9fd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=5, description='Expand Neighborhood Size: ', layout=Layout(width='400px'), max=10, style=Slide…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(widget_terms)\n",
    "display(widget_year_range)\n",
    "display(widget_neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute the Search\n",
    "\n",
    "No need to mode the code manually here. All parameters are grabbed from the widget values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching terms ['duurzaam'] between year 1980 and 1984\n",
      "Found 53 items that match!\n"
     ]
    }
   ],
   "source": [
    "# Unpack values form Widget\n",
    "year_from, year_to = widget_year_range.value\n",
    "years = [str(x) for x in range(year_from, year_to+1)]\n",
    "FILTER_TERMS = [s.strip() for s in widget_terms.value.split(\",\")]\n",
    "# Execute Database Query\n",
    "where_range = {\"year\": {\"$in\": years}}\n",
    "print(f\"Searching terms {FILTER_TERMS} between year {year_from} and {year_to}\")\n",
    "corpus = db.get_corpus(collection, filter_words=FILTER_TERMS, where_obj=where_range, include_embeddings=True, limit=10000)\n",
    "print(f\"Found {len(corpus)} items that match!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute the search for neighborhoods\n",
    "\n",
    "For each `Passage` in the `Corpus` created with the search result, we will find *k* neighbors and add them to the original `Corpus`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Datapoints in the neighborhoods = 265\n",
      "Distance Info: Max = 0.625474214553833 | Min = -1.1920928955078125e-06 | Average = 0.26418773745590785\n",
      "UMAP( verbose=True)\n",
      "Tue May 28 11:22:38 2024 Construct fuzzy simplicial set\n",
      "Tue May 28 11:22:38 2024 Finding Nearest Neighbors\n",
      "Tue May 28 11:22:39 2024 Finished Nearest Neighbor Search\n",
      "Tue May 28 11:22:41 2024 Construct embedding\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c753d2e0900a4a6b8351b68eec582503",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs completed:   0%|            0/500 [00:00]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tcompleted  0  /  500 epochs\n",
      "\tcompleted  50  /  500 epochs\n",
      "\tcompleted  100  /  500 epochs\n",
      "\tcompleted  150  /  500 epochs\n",
      "\tcompleted  200  /  500 epochs\n",
      "\tcompleted  250  /  500 epochs\n",
      "\tcompleted  300  /  500 epochs\n",
      "\tcompleted  350  /  500 epochs\n",
      "\tcompleted  400  /  500 epochs\n",
      "\tcompleted  450  /  500 epochs\n",
      "Tue May 28 11:22:42 2024 Finished embedding\n",
      "(318, 2)\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "def get_corpus_with_neighborhoods(collection, corpus, k_neighbors):\n",
    "    all_neighbors = []\n",
    "    all_distances = []\n",
    "    for p in corpus.passages:\n",
    "        neighbors = db.query_vector_neighbors(collection, vector=p.embedding, k_neighbors=k_neighbors)\n",
    "        for passage, distance in neighbors:\n",
    "            all_neighbors.append(passage)\n",
    "            all_distances.append(distance)\n",
    "    print(f\"Total Datapoints in the neighborhoods = {len(all_neighbors)}\")\n",
    "    print(f\"Distance Info: Max = {max(all_distances)} | Min = {min(all_distances)} | Average = {statistics.mean(all_distances)}\")\n",
    "    # Join original passages + new found neighbors\n",
    "    all_passages = corpus.passages + all_neighbors\n",
    "    corpus = Corpus(all_passages)\n",
    "    corpus.embeddings = db.compress_embeddings(corpus)\n",
    "    print(corpus.embeddings.shape)\n",
    "    return corpus\n",
    "\n",
    "corpus = get_corpus_with_neighborhoods(collection, corpus, k_neighbors=widget_neighbors.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster the Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose Minimum Cluster Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_cluster_size_widget = widgets.IntSlider(\n",
    "    value=10,\n",
    "    min=5,\n",
    "    max=len(corpus)//2,\n",
    "    step=1,\n",
    "    description='Minimum Cluster Size:',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation='horizontal',\n",
    "    readout=True,\n",
    "    readout_format='d',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44a9d228b6c8465b8705821d0b35d45d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=10, continuous_update=False, description='Minimum Cluster Size:', max=159, min=5, style=Slider…"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_cluster_size_widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 clusters in the corpus. (min cluster size is 20)\n",
      "40 Subcorpus(1, [0, 11, 12, 28, 41, 45, 53, 54, 55, 56])\n",
      "38 Subcorpus(6, [1, 3, 6, 19, 31, 35, 39, 49, 58, 59])\n",
      "36 Subcorpus(0, [2, 7, 9, 26, 38, 63, 64, 67, 88, 89])\n",
      "25 Subcorpus(5, [4, 16, 21, 32, 40, 73, 74, 75, 76, 77])\n",
      "22 Subcorpus(4, [5, 20, 52, 66, 78, 80, 91, 95, 153, 154])\n",
      "47 Subcorpus(3, [8, 14, 22, 29, 36, 44, 46, 48, 50, 93])\n",
      "52 Subcorpus(-1, [10, 23, 27, 30, 34, 37, 43, 47, 51, 65])\n",
      "58 Subcorpus(2, [13, 15, 17, 18, 24, 25, 33, 42, 61, 62])\n"
     ]
    }
   ],
   "source": [
    "# Arguments: min_cluster_size=10, cluster_selection_epsilon=0.1, ...\n",
    "# See https://scikit-learn.org/stable/modules/generated/sklearn.cluster.HDBSCAN.html for full list\n",
    "\n",
    "# e.g. min_samples=10, cluster_selection_epsilon=0.2, cluster_selection_method=\"leaf\"\n",
    "clusters = corpus.cluster(min_cluster_size=min_cluster_size_widget.value, cluster_selection_epsilon=0.1)\n",
    "print(f\"Found {len(clusters)} clusters in the corpus. (min cluster size is {min_cluster_size_widget.value})\")\n",
    "\n",
    "if len(clusters) > 100:\n",
    "    raise ValueError(\"Seems like you have too many clusters! Try with a bigger value for min_cluster_size to avoid memory issues\")\n",
    "\n",
    "for c in clusters:\n",
    "    print(len(c.passages), c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Stopwords to avoid including them in the Cluster Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_file = Path(\"stopwords-filter-nl.txt\")\n",
    "\n",
    "with open(stopwords_file.absolute(), \"rt\") as f:\n",
    "    stopwords = set(f.read().splitlines())\n",
    "\n",
    "stopwords.update(\n",
    "    {\n",
    "        \"wij\",\n",
    "        \"we\",\n",
    "        \"moeten\",\n",
    "        \"heer\",\n",
    "        \"mevrouw\",\n",
    "        \"minister\",\n",
    "        \"voorzitter\",\n",
    "        \"gaat\",\n",
    "        \"wel\",\n",
    "        \"den\",\n",
    "    }\n",
    ")\n",
    "\n",
    "%autoreload now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Cluster Passages for inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subcorpus('betere; nabije; ondanks; toekomst; verwachting', [0, 11, 12, 28, 41, 45, 53, 54, 55, 56])\n",
      "Subcorpus('beiroet; maatregelen; regering; vaag; vorming', [1, 31, 35, 49, 58, 59, 60, 208, 209, 210])\n",
      "Subcorpus('afzonderlijke; schip; terreinen; verschillende; zgn', [2, 7, 9, 26, 38, 63, 64, 67, 88, 89])\n",
      "Subcorpus('aup; dsm; industrie; jaarcijfers; toelichting', [3, 6, 19, 39, 68, 69, 70, 71, 83, 84])\n",
      "Subcorpus('antillen; aparte; eilanden; nederlandse; zes', [4, 16, 32, 73, 74, 75, 76, 77, 133, 134])\n",
      "Subcorpus('belancrrijkste; economie; financien; oassen; oeso', [5, 20, 52, 66, 78, 80, 91, 95, 153, 154])\n",
      "Subcorpus('alle; eigen; landen; meeste; overheidsuitgaven', [8, 29, 44, 50, 93, 94, 97, 192, 198, 199])\n",
      "Subcorpus('bezighou; lidstaten; organisatie; vraag; werking', [10, 23, 37, 103, 104, 161, 168, 169, 170, 180])\n",
      "Subcorpus('eind; fonds; gemeenschap; geëindigd; imf', [13, 15, 24, 33, 105, 118, 119, 120, 128, 129])\n",
      "Subcorpus('beschikbaar; gulden; laatste; per; verhoogd', [14, 22, 46, 123, 124, 125, 126, 127, 163, 164])\n",
      "Subcorpus('april; eind; interna; tionale; verslag', [17, 18, 107, 121, 122, 130, 132, 138, 139, 143])\n",
      "Subcorpus('deor; enigde; staten; sterke; wereldhande', [21, 40, 43, 51, 79, 81, 158, 159, 162, 253])\n",
      "Subcorpus('beroep; betrokken; dringend; komen; territoriale', [25, 42, 61, 62, 106, 178, 179, 211, 212, 231])\n",
      "Subcorpus('hand; houden; opleven; termijn; verder', [27, 30, 100, 188, 189, 202, 203, 204, 227, 276])\n",
      "Subcorpus('financien; nassen; onc; volaens; èerp', [34, 47, 65, 82, 90, 96, 102, 155, 190, 206])\n",
      "Subcorpus('commissie; durft; kansen; odder; regets', [36, 48, 187, 233, 234, 247, 293, 294, 301, 302])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if not os.path.exists(\"clusters\"): \n",
    "    os.makedirs(\"clusters\")\n",
    "\n",
    "selected_metadata = [\"year\"]\n",
    "\n",
    "all_clusters_records, df_cluster_labels, df_cluster_meta = [], [], []\n",
    "for cluster in clusters:\n",
    "    cluster.set_topic_label(exclude_words=frozenset(stopwords | set(FILTER_TERMS)), n=5)\n",
    "    df = cluster.to_dataframe()\n",
    "    centroid = cluster.centroid()\n",
    "    label = cluster.label\n",
    "    cluster_size = len(cluster.passages)\n",
    "    # Compute Cluster Stats as a Subcorpus\n",
    "    top_words = \" \".join(cluster.top_words(exclude_words=frozenset(stopwords | set(FILTER_TERMS)), n=10))\n",
    "    all_clusters_records.append((f\"{label}\\t{cluster_size}\\t{centroid}\\t{top_words}\\n\"))\n",
    "    df_cluster_labels.append(cluster.label)\n",
    "    df_cluster_meta.append(df[selected_metadata])\n",
    "    # Save the Cluster Passages in a File\n",
    "    file_prefix = f\"cluster_{year_from}_{year_to}_{cluster.label.replace('; ', '_')}\"\n",
    "    df.to_csv(f\"clusters/{file_prefix}.tsv\", sep=\"\\t\", index=False) \n",
    "    print(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"clusters/clusters_all_{year_from}_{year_to}.csv\", \"wt\") as f:\n",
    "    f.write(\"Label\\tSize\\tCentroid\\tTopWords\\n\")\n",
    "    for rec in all_clusters_records:\n",
    "        f.write(rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Cluster Content Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14e9a6194f874a0bba4a4d9d570d3f02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Choose a Cluster:', options=(('betere; nabije; ondanks; toekomst; …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_cluster_distribution(cluster_index, column_name)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "logging.getLogger('matplotlib').setLevel(logging.WARNING)\n",
    "from ipywidgets import interact\n",
    "\n",
    "def plot_cluster_distribution(cluster_index, column_name):\n",
    "\n",
    "    df = df_cluster_meta[cluster_index]\n",
    "    cluster_name = df_cluster_labels[cluster_index]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(df[column_name],edgecolor='black')\n",
    "    plt.xlabel(f'{column_name}', fontsize=14)\n",
    "    plt.ylabel('Frequency', fontsize=14)\n",
    "    plt.title(f'{cluster_name}', fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "cluster_selector = widgets.Dropdown(\n",
    "    options=[(lbl, i) for i, lbl in enumerate(df_cluster_labels)],\n",
    "    description='Choose a Cluster:',\n",
    "    disabled=False,\n",
    "    style={'description_width': 'initial'} \n",
    ")\n",
    "\n",
    "variable_selector = widgets.Dropdown(\n",
    "    options=selected_metadata,\n",
    "    description='Choose a Column to Plot:',\n",
    "    disabled=False,\n",
    "    style={'description_width': 'initial'} \n",
    ")\n",
    "\n",
    "interact(plot_cluster_distribution, cluster_index=cluster_selector, column_name=variable_selector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Embeddings (All Clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.plotting import show\n",
    "from tempo_embeddings.visualization.bokeh import BokehInteractiveVisualizer\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "meta_fields = corpus.metadata_fields()\n",
    "meta_fields = [\"year\", \"date\", \"issue\"]\n",
    "\n",
    "visualizer = BokehInteractiveVisualizer(\n",
    "    *clusters, metadata_fields=meta_fields, width=2000, height=1000\n",
    ")\n",
    "\n",
    "os.environ[\n",
    "    \"BOKEH_ALLOW_WS_ORIGIN\"\n",
    "] = \"*\"\n",
    "\n",
    "show(visualizer.create_document)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
