{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install tempo-embeddings from GitHub\n",
    "# # This can also refer to a specific version or branch\n",
    "\n",
    "# %pip install --upgrade pip  # Required for properly resolving dependencies\n",
    "# %pip uninstall -y tempo_embeddings  # Remove existing installation\n",
    "# %pip install --upgrade git+https://github.com/Semantics-of-Sustainability/tempo-embeddings.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure installation has succeeded\n",
    "import tempo_embeddings\n",
    "\n",
    "from importlib import reload\n",
    "import logging\n",
    "reload(logging)\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s:%(message)s', level=logging.INFO, datefmt='%I:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "\n",
    "    IN_COLAB = True\n",
    "except ModuleNotFoundError:\n",
    "    IN_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload now\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from tempo_embeddings.text.corpus import Corpus\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Database Manager\n",
    "\n",
    "The `db_path` parameter should point to the directory where the database is, so the original configuration and records are loaded. The database was created using the notebook `1_compute_embeddings_nl.ipynb`. If the given path does not exist, a new EMPTY database will be created there. \n",
    "\n",
    "A bigger `batch_size` could make the search faster but if it is too big you might run out of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jose/Repos/tempo-embeddings/.venv/lib/python3.9/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "03:07:45 INFO:Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "03:07:45 INFO:Path 'testing_db' already exists. Loading DB configuration:\n",
      "{'embedder_name': 'NetherlandsForensicInstitute/robbert-2022-dutch-sentence-transformers', 'embedder_type': 'custom_model', 'existing_collections': ['anp_sg_corpus', 'anp_duurzam']}\n"
     ]
    }
   ],
   "source": [
    "from tempo_embeddings.embeddings.vector_database import ChromaDatabaseManager\n",
    "\n",
    "# Here we load only the ANP collection because metadata field names diverge across datasets\n",
    "db = ChromaDatabaseManager(db_path=\"testing_db\", batch_size=8)\n",
    "db.connect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose from the available Collections in the Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5704e925d6c7413687407e75ba0a7043",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Choose a Collection:', options=('anp_sg_corpus', 'anp_duurzam'), style=DescriptionStyle(…"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "existing_colls = db.get_available_collections()\n",
    "collection_selector = widgets.Dropdown(\n",
    "    options=existing_colls,\n",
    "    description='Choose a Collection:',\n",
    "    disabled=False,\n",
    "    style={'description_width': 'initial'} \n",
    ")\n",
    "collection_selector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show number of records in the selected collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03:07:45 INFO:Retrieved existing collection 'anp_sg_corpus'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collection 'anp_sg_corpus' has 1558 records\n"
     ]
    }
   ],
   "source": [
    "collection = db.get_existing_collection(collection_selector.value)\n",
    "print(f\"\\nCollection '{collection_selector.value}' has {collection.count()} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Sub-Corpus\n",
    "\n",
    "To make the processing and visualization easier, we will create a new `Corpus` comprising only a subet of the original Collection. This corpus will contain only the records of interest. This is done by querying the database with keyword and metadata constraints. In this example we allow to look for:\n",
    "\n",
    "- **Filter Terms:** retrieve only passages that contain exactly the given keywords.\n",
    "- **Year Range:** retrieve only the records which are inside the provided years\n",
    "- **Neighbors:** This indicates how much to *expand* the search into more datapoints. The idea is to retrieve the *top_k* neighbors of the initially retrieved passages. Ideally this will give related passages that did not mention any of the keywords explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "widget_year_range=widgets.IntRangeSlider(description='Year Range: ', min=1900, max=2020, step=1, value=(1980,1984), style={'description_width': 'initial'}, layout=widgets.Layout(width='400px') )\n",
    "widget_terms=widgets.Text(description='Filter Terms (comma separated)', value=\"duurzaam\", style={'description_width': 'initial'}, layout=widgets.Layout(width='600px') )\n",
    "widget_neighbors=widgets.IntSlider(description=\"Expand Neighborhood Size: \", min=0, max=10, value=5, style={'description_width': 'initial'}, layout=widgets.Layout(width='400px') )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the Widgets to choose the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73720b3d9abf4ee588d6c9cbde9b22ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='duurzaam', description='Filter Terms (comma separated)', layout=Layout(width='600px'), style=TextS…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d98a083cfe64b619486e17daa1bc728",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntRangeSlider(value=(1980, 1984), description='Year Range: ', layout=Layout(width='400px'), max=2020, min=190…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a5c8752649a4489ad00e408362376ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=5, description='Expand Neighborhood Size: ', layout=Layout(width='400px'), max=10, style=Slide…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(widget_terms)\n",
    "display(widget_year_range)\n",
    "display(widget_neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute the Search\n",
    "\n",
    "No need to move the code manually here. All parameters are grabbed from the widget values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching terms ['duurzaam'] between year 1980 and 2020\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected where to have exactly one operator, got {'year_from': 1980, 'year_to': 2020}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m where_range \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myear_from\u001b[39m\u001b[38;5;124m\"\u001b[39m: year_from, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myear_to\u001b[39m\u001b[38;5;124m\"\u001b[39m: year_to}\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSearching terms \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mFILTER_TERMS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m between year \u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear_from\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear_to\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[43mdb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_corpus\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcollection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilter_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFILTER_TERMS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere_obj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere_range\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(corpus)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m items that match!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Repos/tempo-embeddings/.venv/lib/python3.9/site-packages/tempo_embeddings/embeddings/vector_database.py:297\u001b[0m, in \u001b[0;36mChromaDatabaseManager.get_corpus\u001b[0;34m(self, collection, filter_words, where_obj, limit, include_embeddings)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m limit \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    296\u001b[0m     limit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 297\u001b[0m records \u001b[38;5;241m=\u001b[39m \u001b[43mcollection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere_document\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere_doc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;66;03m# Return empty corpus if no records where found\u001b[39;00m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(records) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Repos/tempo-embeddings/.venv/lib/python3.9/site-packages/chromadb/api/models/Collection.py:195\u001b[0m, in \u001b[0;36mCollection.get\u001b[0;34m(self, ids, where, limit, offset, where_document, include)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    172\u001b[0m     ids: Optional[OneOrMany[ID]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    177\u001b[0m     include: Include \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadatas\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocuments\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    178\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m GetResult:\n\u001b[1;32m    179\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get embeddings and their associate data from the data store. If no ids or where filter is provided returns\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;124;03m    all embeddings up to limit starting at offset.\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    192\u001b[0m \n\u001b[1;32m    193\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 195\u001b[0m     valid_where \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_where\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m where \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    196\u001b[0m     valid_where_document \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    197\u001b[0m         validate_where_document(where_document) \u001b[38;5;28;01mif\u001b[39;00m where_document \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    198\u001b[0m     )\n\u001b[1;32m    199\u001b[0m     valid_ids \u001b[38;5;241m=\u001b[39m validate_ids(maybe_cast_one_to_many_ids(ids)) \u001b[38;5;28;01mif\u001b[39;00m ids \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Repos/tempo-embeddings/.venv/lib/python3.9/site-packages/chromadb/api/types.py:303\u001b[0m, in \u001b[0;36mvalidate_where\u001b[0;34m(where)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected where to be a dict, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwhere\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(where) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 303\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected where to have exactly one operator, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwhere\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m where\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[0;31mValueError\u001b[0m: Expected where to have exactly one operator, got {'year_from': 1980, 'year_to': 2020}"
     ]
    }
   ],
   "source": [
    "# Unpack values form Widget\n",
    "year_from, year_to = widget_year_range.value\n",
    "years = [str(x) for x in range(year_from, year_to+1)]\n",
    "FILTER_TERMS = [s.strip() for s in widget_terms.value.split(\",\")]\n",
    "# Execute Database Query\n",
    "where_range = {\"year\": {\"$in\": years}}\n",
    "print(f\"Searching terms {FILTER_TERMS} between year {year_from} and {year_to}\")\n",
    "corpus = db.get_corpus(collection, filter_words=FILTER_TERMS, where_obj=where_range, include_embeddings=True, limit=10000)\n",
    "print(f\"Found {len(corpus)} items that match!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute the search for neighborhoods\n",
    "\n",
    "For each `Passage` in the `Corpus` created with the search result, we will find *k* neighbors and add them to the original `Corpus`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "def get_corpus_with_neighborhoods(collection, corpus, k_neighbors):\n",
    "    all_neighbors = []\n",
    "    all_distances = []\n",
    "    for p in corpus.passages:\n",
    "        neighbors = db.query_vector_neighbors(collection, vector=p.embedding, k_neighbors=k_neighbors)\n",
    "        for passage, distance in neighbors:\n",
    "            all_neighbors.append(passage)\n",
    "            all_distances.append(distance)\n",
    "    print(f\"Total Datapoints in the neighborhoods = {len(all_neighbors)}\")\n",
    "    print(f\"Distance Info: Max = {max(all_distances)} | Min = {min(all_distances)} | Average = {statistics.mean(all_distances)}\")\n",
    "    # Join original passages + new found neighbors\n",
    "    all_passages = corpus.passages + all_neighbors\n",
    "    corpus = Corpus(all_passages)\n",
    "    corpus.embeddings = db.compress_embeddings(corpus)\n",
    "    print(corpus.embeddings.shape)\n",
    "    return corpus\n",
    "\n",
    "corpus = get_corpus_with_neighborhoods(collection, corpus, k_neighbors=widget_neighbors.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster the Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose Minimum Cluster Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_cluster_size_widget = widgets.IntSlider(\n",
    "    value=10,\n",
    "    min=5,\n",
    "    max=len(corpus)//2,\n",
    "    step=1,\n",
    "    description='Minimum Cluster Size:',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation='horizontal',\n",
    "    readout=True,\n",
    "    readout_format='d',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_cluster_size_widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments: min_cluster_size=10, cluster_selection_epsilon=0.1, ...\n",
    "# See https://scikit-learn.org/stable/modules/generated/sklearn.cluster.HDBSCAN.html for full list\n",
    "\n",
    "# e.g. min_samples=10, cluster_selection_epsilon=0.2, cluster_selection_method=\"leaf\"\n",
    "clusters = corpus.cluster(min_cluster_size=min_cluster_size_widget.value, cluster_selection_epsilon=0.1)\n",
    "print(f\"Found {len(clusters)} clusters in the corpus. (min cluster size is {min_cluster_size_widget.value})\")\n",
    "\n",
    "if len(clusters) > 100:\n",
    "    raise ValueError(\"Seems like you have too many clusters! Try with a bigger value for min_cluster_size to avoid memory issues\")\n",
    "\n",
    "for c in clusters:\n",
    "    print(len(c.passages), c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Stopwords to avoid including them in the Cluster Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_file = Path(\"stopwords-filter-nl.txt\")\n",
    "\n",
    "with open(stopwords_file.absolute(), \"rt\") as f:\n",
    "    stopwords = set(f.read().splitlines())\n",
    "\n",
    "stopwords.update(\n",
    "    {\n",
    "        \"wij\",\n",
    "        \"we\",\n",
    "        \"moeten\",\n",
    "        \"heer\",\n",
    "        \"mevrouw\",\n",
    "        \"minister\",\n",
    "        \"voorzitter\",\n",
    "        \"gaat\",\n",
    "        \"wel\",\n",
    "        \"den\",\n",
    "    }\n",
    ")\n",
    "\n",
    "%autoreload now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Cluster Passages for inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(\"clusters\"): \n",
    "    os.makedirs(\"clusters\")\n",
    "\n",
    "selected_metadata = [\"year\"]\n",
    "\n",
    "all_clusters_records, df_cluster_labels, df_cluster_meta = [], [], []\n",
    "for cluster in clusters:\n",
    "    cluster.set_topic_label(exclude_words=frozenset(stopwords | set(FILTER_TERMS)), n=5)\n",
    "    df = cluster.to_dataframe()\n",
    "    centroid = cluster.centroid()\n",
    "    label = cluster.label\n",
    "    cluster_size = len(cluster.passages)\n",
    "    # Compute Cluster Stats as a Subcorpus\n",
    "    top_words = \" \".join(cluster.top_words(exclude_words=frozenset(stopwords | set(FILTER_TERMS)), n=10))\n",
    "    all_clusters_records.append((f\"{label}\\t{cluster_size}\\t{centroid}\\t{top_words}\\n\"))\n",
    "    df_cluster_labels.append(cluster.label)\n",
    "    df_cluster_meta.append(df[selected_metadata])\n",
    "    # Save the Cluster Passages in a File\n",
    "    file_prefix = f\"cluster_{year_from}_{year_to}_{cluster.label.replace('; ', '_')}\"\n",
    "    df.to_csv(f\"clusters/{file_prefix}.tsv\", sep=\"\\t\", index=False) \n",
    "    print(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"clusters/clusters_all_{year_from}_{year_to}.csv\", \"wt\") as f:\n",
    "    f.write(\"Label\\tSize\\tCentroid\\tTopWords\\n\")\n",
    "    for rec in all_clusters_records:\n",
    "        f.write(rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Cluster Content Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "logging.getLogger('matplotlib').setLevel(logging.WARNING)\n",
    "from ipywidgets import interact\n",
    "\n",
    "def plot_cluster_distribution(cluster_index, column_name):\n",
    "\n",
    "    df = df_cluster_meta[cluster_index]\n",
    "    cluster_name = df_cluster_labels[cluster_index]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(df[column_name],edgecolor='black')\n",
    "    plt.xlabel(f'{column_name}', fontsize=14)\n",
    "    plt.ylabel('Frequency', fontsize=14)\n",
    "    plt.title(f'{cluster_name}', fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "cluster_selector = widgets.Dropdown(\n",
    "    options=[(lbl, i) for i, lbl in enumerate(df_cluster_labels)],\n",
    "    description='Choose a Cluster:',\n",
    "    disabled=False,\n",
    "    style={'description_width': 'initial'} \n",
    ")\n",
    "\n",
    "variable_selector = widgets.Dropdown(\n",
    "    options=selected_metadata,\n",
    "    description='Choose a Column to Plot:',\n",
    "    disabled=False,\n",
    "    style={'description_width': 'initial'} \n",
    ")\n",
    "\n",
    "interact(plot_cluster_distribution, cluster_index=cluster_selector, column_name=variable_selector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Embeddings (All Clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.plotting import show\n",
    "from tempo_embeddings.visualization.bokeh import BokehInteractiveVisualizer\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "meta_fields = corpus.metadata_fields()\n",
    "meta_fields = [\"year\", \"date\", \"issue\"]\n",
    "\n",
    "visualizer = BokehInteractiveVisualizer(\n",
    "    *clusters, metadata_fields=meta_fields, width=2000, height=1000\n",
    ")\n",
    "\n",
    "os.environ[\n",
    "    \"BOKEH_ALLOW_WS_ORIGIN\"\n",
    "] = \"*\"\n",
    "\n",
    "show(visualizer.create_document)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
